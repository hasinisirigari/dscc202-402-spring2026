{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Transformations vs Actions - Solutions\n",
    "\n",
    "**Objective**: Master the fundamental distinction between transformations (lazy) and actions (eager) in Apache Spark.\n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Understand lazy evaluation and when transformations are executed\n",
    "- Identify transformations vs actions in Spark operations\n",
    "- Observe execution timing and DAG construction\n",
    "- Apply performance optimization techniques\n",
    "- Debug execution plans and understand job stages\n",
    "\n",
    "**Estimated Time**: 50 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark Session with detailed logging\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab2-Transformations-vs-Actions\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")  # Suppress warnings for cleaner output\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "# Enhanced Spark UI URL display\n",
    "ui_url = spark.sparkContext.uiWebUrl\n",
    "print(f\"Spark UI: {ui_url}\")\n",
    "print(\"üí° In GitHub Codespaces: Check the 'PORTS' tab below for forwarded port 4040 to access Spark UI\")\n",
    "\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Transformations\n",
    "\n",
    "Transformations are **lazy operations** that define a new RDD/DataFrame but don't compute results immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Lazy Evaluation Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our transaction data\n",
    "transactions_rdd = sc.textFile(\"../Datasets/customer_transactions.csv\")\n",
    "header = transactions_rdd.first()\n",
    "transactions_no_header = transactions_rdd.filter(lambda line: line != header)\n",
    "\n",
    "print(\"Data loaded. Let's observe lazy evaluation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain of transformations (all lazy!)\n",
    "print(\"Creating transformation chain...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Parse transaction data\n",
    "def parse_transaction(line):\n",
    "    fields = line.split(',')\n",
    "    return {\n",
    "        'transaction_id': fields[0],\n",
    "        'customer_id': fields[1],\n",
    "        'amount': float(fields[2]),\n",
    "        'category': fields[3],\n",
    "        'payment_method': fields[4]\n",
    "    }\n",
    "\n",
    "parsed_transactions = transactions_no_header.map(parse_transaction)\n",
    "print(f\"‚úì Map transformation defined - Time: {time.time() - start_time:.4f}s\")\n",
    "\n",
    "# Step 2: Filter for high-value transactions\n",
    "high_value = parsed_transactions.filter(lambda t: t['amount'] > 100)\n",
    "print(f\"‚úì Filter transformation defined - Time: {time.time() - start_time:.4f}s\")\n",
    "\n",
    "# Step 3: Extract amounts only\n",
    "amounts_only = high_value.map(lambda t: t['amount'])\n",
    "print(f\"‚úì Second map transformation defined - Time: {time.time() - start_time:.4f}s\")\n",
    "\n",
    "# Step 4: Create key-value pairs for category analysis\n",
    "category_amounts = parsed_transactions.map(lambda t: (t['category'], t['amount']))\n",
    "print(f\"‚úì Category mapping transformation defined - Time: {time.time() - start_time:.4f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nüéØ All transformations defined in {total_time:.4f} seconds\")\n",
    "print(\"üìù Notice: No actual computation has happened yet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1**: Create your own transformation chain and measure definition time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Create a transformation pipeline that:\n",
    "# 1. Filters for Electronics category\n",
    "# 2. Maps to extract customer_id and amount as tuple\n",
    "# 3. Filters for amounts between $50-$500\n",
    "# 4. Maps to create (customer_id, amount^2) pairs\n",
    "\n",
    "print(\"Creating your transformation chain...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Filter for Electronics\n",
    "electronics_only = parsed_transactions.filter(lambda t: t['category'] == 'Electronics')\n",
    "print(f\"‚úì Electronics filter defined - Time: {time.time() - start_time:.4f}s\")\n",
    "\n",
    "# Step 2: Extract customer_id and amount\n",
    "customer_amount_pairs = electronics_only.map(lambda t: (t['customer_id'], t['amount']))\n",
    "print(f\"‚úì Customer-amount mapping defined - Time: {time.time() - start_time:.4f}s\")\n",
    "\n",
    "# Step 3: Filter for amount range $50-$500\n",
    "amount_range_filtered = customer_amount_pairs.filter(lambda x: 50 <= x[1] <= 500)\n",
    "print(f\"‚úì Amount range filter defined - Time: {time.time() - start_time:.4f}s\")\n",
    "\n",
    "# Step 4: Square the amounts\n",
    "squared_amounts = amount_range_filtered.map(lambda x: (x[0], x[1] ** 2))\n",
    "print(f\"‚úì Squared amounts mapping defined - Time: {time.time() - start_time:.4f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nüéØ Your transformation chain defined in {total_time:.4f} seconds\")\n",
    "\n",
    "# Validation (this will be the first action!)\n",
    "sample_result = squared_amounts.take(1)\n",
    "assert len(sample_result) > 0, \"Should have at least one result\"\n",
    "assert len(sample_result[0]) == 2, \"Should be (customer_id, amount^2) pairs\"\n",
    "print(\"‚úì Exercise 1.1 completed successfully!\")\n",
    "print(f\"Sample result: {sample_result[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Examining the Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the lineage without triggering execution\n",
    "print(\"Execution plan for amounts_only RDD:\")\n",
    "print(amounts_only.toDebugString().decode('utf-8'))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Number of partitions:\", amounts_only.getNumPartitions())\n",
    "print(\"Storage level:\", amounts_only.getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Actions\n",
    "\n",
    "Actions are **eager operations** that trigger the execution of the entire transformation chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Triggering Execution with Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's trigger execution with various actions\n",
    "print(\"Triggering execution with actions...\\n\")\n",
    "\n",
    "# Action 1: count()\n",
    "print(\"1. Executing count() action...\")\n",
    "start_time = time.time()\n",
    "count_result = amounts_only.count()\n",
    "count_time = time.time() - start_time\n",
    "print(f\"   Result: {count_result} high-value transactions\")\n",
    "print(f\"   Execution time: {count_time:.4f}s\\n\")\n",
    "\n",
    "# Action 2: first()\n",
    "print(\"2. Executing first() action...\")\n",
    "start_time = time.time()\n",
    "first_result = amounts_only.first()\n",
    "first_time = time.time() - start_time\n",
    "print(f\"   Result: ${first_result:.2f}\")\n",
    "print(f\"   Execution time: {first_time:.4f}s\\n\")\n",
    "\n",
    "# Action 3: take(5)\n",
    "print(\"3. Executing take(5) action...\")\n",
    "start_time = time.time()\n",
    "take_result = amounts_only.take(5)\n",
    "take_time = time.time() - start_time\n",
    "print(f\"   Result: {[f'${x:.2f}' for x in take_result]}\")\n",
    "print(f\"   Execution time: {take_time:.4f}s\\n\")\n",
    "\n",
    "# Action 4: collect() - be careful with large datasets!\n",
    "print(\"4. Executing collect() action (first 100 elements)...\")\n",
    "start_time = time.time()\n",
    "# Limit collection to avoid memory issues\n",
    "limited_collect = amounts_only.take(100)  # Safer than collect()\n",
    "collect_time = time.time() - start_time\n",
    "print(f\"   Collected {len(limited_collect)} elements\")\n",
    "print(f\"   Execution time: {collect_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: Compare execution times of different actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Execute different actions on category_amounts and measure timing\n",
    "execution_times = {}\n",
    "\n",
    "# Action 1: Count unique categories\n",
    "print(\"Measuring action execution times...\")\n",
    "\n",
    "start_time = time.time()\n",
    "unique_categories_count = category_amounts.keys().distinct().count()\n",
    "execution_times['distinct_count'] = time.time() - start_time\n",
    "print(f\"Unique categories: {unique_categories_count} (Time: {execution_times['distinct_count']:.4f}s)\")\n",
    "\n",
    "# Action 2: Calculate total by category using reduceByKey + collect\n",
    "start_time = time.time()\n",
    "category_totals = category_amounts.reduceByKey(lambda a, b: a + b).collect()\n",
    "execution_times['reduce_collect'] = time.time() - start_time\n",
    "print(f\"Category totals calculated (Time: {execution_times['reduce_collect']:.4f}s)\")\n",
    "\n",
    "# Action 3: Find maximum amount using reduce\n",
    "start_time = time.time()\n",
    "max_amount = category_amounts.values().reduce(lambda a, b: max(a, b))\n",
    "execution_times['max_reduce'] = time.time() - start_time\n",
    "print(f\"Maximum amount: ${max_amount:.2f} (Time: {execution_times['max_reduce']:.4f}s)\")\n",
    "\n",
    "# Action 4: Sample data\n",
    "start_time = time.time()\n",
    "sample_data = category_amounts.sample(False, 0.1).count()\n",
    "execution_times['sample_count'] = time.time() - start_time\n",
    "print(f\"Sample count: {sample_data} (Time: {execution_times['sample_count']:.4f}s)\")\n",
    "\n",
    "# Validation\n",
    "assert unique_categories_count > 0, \"Should have at least one category\"\n",
    "assert len(category_totals) > 0, \"Should have category totals\"\n",
    "assert max_amount > 0, \"Max amount should be positive\"\n",
    "\n",
    "print(\"\\n‚úì Exercise 2.1 completed successfully!\")\n",
    "print(\"\\nExecution Time Summary:\")\n",
    "for action, time_taken in execution_times.items():\n",
    "    print(f\"  {action}: {time_taken:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Actions That Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions that save data to external storage\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Create temporary directory for outputs\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Using temporary directory: {temp_dir}\")\n",
    "\n",
    "# Save high-value transactions\n",
    "output_path = os.path.join(temp_dir, \"high_value_amounts\")\n",
    "\n",
    "print(\"\\nSaving data with actions...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# This is an action that triggers execution and writes to disk\n",
    "amounts_only.saveAsTextFile(output_path)\n",
    "\n",
    "save_time = time.time() - start_time\n",
    "print(f\"Data saved in {save_time:.4f}s\")\n",
    "\n",
    "# Verify the save worked\n",
    "saved_files = os.listdir(output_path)\n",
    "print(f\"Files created: {len([f for f in saved_files if f.startswith('part-')])} partition files\")\n",
    "\n",
    "# Clean up\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "print(\"Temporary files cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Performance Implications\n",
    "\n",
    "Understanding when computation happens is crucial for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Multiple Actions on Same RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate recomputation without caching\n",
    "print(\"Testing recomputation behavior...\\n\")\n",
    "\n",
    "# Create a complex RDD that takes time to compute\n",
    "complex_rdd = parsed_transactions \\\n",
    "    .filter(lambda t: t['amount'] > 50) \\\n",
    "    .map(lambda t: (t['category'], t['amount'])) \\\n",
    "    .filter(lambda x: x[1] < 500)\n",
    "\n",
    "print(\"üîÑ Without caching - each action recomputes the entire chain:\")\n",
    "\n",
    "# First action\n",
    "start_time = time.time()\n",
    "count1 = complex_rdd.count()\n",
    "time1 = time.time() - start_time\n",
    "print(f\"   First count(): {count1} records in {time1:.4f}s\")\n",
    "\n",
    "# Second action - recomputes everything!\n",
    "start_time = time.time()\n",
    "first_elem = complex_rdd.first()\n",
    "time2 = time.time() - start_time\n",
    "print(f\"   First element: {first_elem} in {time2:.4f}s\")\n",
    "\n",
    "# Third action - recomputes again!\n",
    "start_time = time.time()\n",
    "sample_data = complex_rdd.take(10)\n",
    "time3 = time.time() - start_time\n",
    "print(f\"   Sample (10 elements) retrieved in {time3:.4f}s\")\n",
    "\n",
    "total_time_no_cache = time1 + time2 + time3\n",
    "print(f\"\\nüìä Total time without caching: {total_time_no_cache:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Caching for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's cache the RDD and see the difference\n",
    "print(\"\\nüíæ With caching - computation happens once:\")\n",
    "\n",
    "# Cache the RDD\n",
    "complex_rdd.cache()\n",
    "\n",
    "# First action - computes and caches\n",
    "start_time = time.time()\n",
    "count1_cached = complex_rdd.count()\n",
    "time1_cached = time.time() - start_time\n",
    "print(f\"   First count() (with caching): {count1_cached} in {time1_cached:.4f}s\")\n",
    "\n",
    "# Second action - uses cache!\n",
    "start_time = time.time()\n",
    "first_elem_cached = complex_rdd.first()\n",
    "time2_cached = time.time() - start_time\n",
    "print(f\"   First element (from cache): {first_elem_cached} in {time2_cached:.4f}s\")\n",
    "\n",
    "# Third action - uses cache!\n",
    "start_time = time.time()\n",
    "sample_cached = complex_rdd.take(10)\n",
    "time3_cached = time.time() - start_time\n",
    "print(f\"   Sample (from cache): retrieved in {time3_cached:.4f}s\")\n",
    "\n",
    "total_time_cached = time1_cached + time2_cached + time3_cached\n",
    "print(f\"\\nüìä Total time with caching: {total_time_cached:.4f}s\")\n",
    "print(f\"üöÄ Performance improvement: {((total_time_no_cache - total_time_cached) / total_time_no_cache * 100):.1f}%\")\n",
    "\n",
    "# Check cache status\n",
    "print(f\"\\nüìã Cache info:\")\n",
    "print(f\"   Is cached: {complex_rdd.is_cached}\")\n",
    "print(f\"   Storage level: {complex_rdd.getStorageLevel()}\")\n",
    "\n",
    "# Clean up cache\n",
    "complex_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1**: Implement your own caching performance test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Create a performance test comparing cached vs uncached operations\n",
    "\n",
    "# Step 1: Create a computationally expensive RDD\n",
    "expensive_rdd = parsed_transactions \\\n",
    "    .filter(lambda t: t['amount'] > 25) \\\n",
    "    .map(lambda t: (t['customer_id'], t['amount'] * 1.1)) \\\n",
    "    .filter(lambda x: x[1] > 30)\n",
    "\n",
    "print(\"üß™ Performance Test: Cached vs Uncached\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test WITHOUT caching\n",
    "print(\"\\nüìà WITHOUT CACHING:\")\n",
    "uncached_times = []\n",
    "\n",
    "for i in range(3):\n",
    "    start_time = time.time()\n",
    "    if i == 0:\n",
    "        result = expensive_rdd.count()\n",
    "    elif i == 1:\n",
    "        result = expensive_rdd.take(5)\n",
    "    else:\n",
    "        result = expensive_rdd.map(lambda x: x[1]).max()\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    uncached_times.append(execution_time)\n",
    "    print(f\"   Operation {i+1}: {execution_time:.4f}s\")\n",
    "\n",
    "# Test WITH caching\n",
    "print(\"\\nüíæ WITH CACHING:\")\n",
    "expensive_rdd.cache()\n",
    "cached_times = []\n",
    "\n",
    "for i in range(3):\n",
    "    start_time = time.time()\n",
    "    if i == 0:\n",
    "        result = expensive_rdd.count()\n",
    "    elif i == 1:\n",
    "        result = expensive_rdd.take(5)\n",
    "    else:\n",
    "        result = expensive_rdd.map(lambda x: x[1]).max()\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    cached_times.append(execution_time)\n",
    "    print(f\"   Operation {i+1}: {execution_time:.4f}s\")\n",
    "\n",
    "# Calculate improvements\n",
    "total_uncached = sum(uncached_times)\n",
    "total_cached = sum(cached_times)\n",
    "improvement = ((total_uncached - total_cached) / total_uncached) * 100\n",
    "\n",
    "print(f\"\\nüìä RESULTS:\")\n",
    "print(f\"   Total time without cache: {total_uncached:.4f}s\")\n",
    "print(f\"   Total time with cache: {total_cached:.4f}s\")\n",
    "print(f\"   Performance improvement: {improvement:.1f}%\")\n",
    "\n",
    "# Validation\n",
    "assert expensive_rdd.is_cached, \"RDD should be cached\"\n",
    "# Note: For small educational datasets, caching overhead may exceed benefits\n",
    "# In production with larger datasets, caching typically provides significant improvements\n",
    "if improvement > 0:\n",
    "    print(f\"üöÄ Caching provided {improvement:.1f}% improvement!\")\n",
    "else:\n",
    "    print(f\"üìö Educational note: Small datasets may show caching overhead ({improvement:.1f}%)\")\n",
    "    print(\"   In production with larger datasets, caching typically provides significant benefits\")\n",
    "\n",
    "print(\"\\n‚úì Exercise 3.1 completed successfully!\")\n",
    "\n",
    "# Cleanup\n",
    "expensive_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Understanding Job Execution\n",
    "\n",
    "Let's examine how Spark breaks down our operations into jobs, stages, and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Simple vs Complex Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple operation - narrow transformations only\n",
    "print(\"üîç Analyzing simple operation (narrow transformations):\")\n",
    "simple_rdd = parsed_transactions \\\n",
    "    .filter(lambda t: t['amount'] > 100) \\\n",
    "    .map(lambda t: t['amount'])\n",
    "\n",
    "print(f\"Lineage for simple operation:\")\n",
    "print(simple_rdd.toDebugString().decode('utf-8'))\n",
    "\n",
    "# Execute and time\n",
    "start_time = time.time()\n",
    "simple_count = simple_rdd.count()\n",
    "simple_time = time.time() - start_time\n",
    "print(f\"\\nSimple operation result: {simple_count} records in {simple_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex operation - includes wide transformation (shuffle)\n",
    "print(\"\\nüîÄ Analyzing complex operation (includes shuffle):\")\n",
    "complex_rdd = parsed_transactions \\\n",
    "    .map(lambda t: (t['category'], t['amount'])) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(f\"Lineage for complex operation:\")\n",
    "print(complex_rdd.toDebugString().decode('utf-8'))\n",
    "\n",
    "# Execute and time\n",
    "start_time = time.time()\n",
    "complex_result = complex_rdd.collect()\n",
    "complex_time = time.time() - start_time\n",
    "print(f\"\\nComplex operation result: {len(complex_result)} categories in {complex_time:.4f}s\")\n",
    "print(f\"Top 3 categories by total amount:\")\n",
    "for category, amount in complex_result[:3]:\n",
    "    print(f\"  {category}: ${amount:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.1**: Compare narrow vs wide transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Create and compare narrow vs wide transformation pipelines\n",
    "\n",
    "print(\"üî¨ Comparing Narrow vs Wide Transformations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Narrow transformation pipeline (no shuffle)\n",
    "print(\"\\nüìè NARROW TRANSFORMATIONS (no data movement):\")\n",
    "narrow_pipeline = parsed_transactions \\\n",
    "    .filter(lambda t: t['category'] == 'Electronics') \\\n",
    "    .map(lambda t: t['amount'] * 2) \\\n",
    "    .filter(lambda amount: amount > 100)\n",
    "\n",
    "# Time narrow operations\n",
    "start_time = time.time()\n",
    "narrow_count = narrow_pipeline.count()\n",
    "narrow_max = narrow_pipeline.max()\n",
    "narrow_time = time.time() - start_time\n",
    "\n",
    "print(f\"   Count: {narrow_count}\")\n",
    "print(f\"   Max: ${narrow_max:.2f}\")\n",
    "print(f\"   Execution time: {narrow_time:.4f}s\")\n",
    "print(f\"   Partitions: {narrow_pipeline.getNumPartitions()}\")\n",
    "\n",
    "# Wide transformation pipeline (includes shuffle)\n",
    "print(\"\\nüåê WIDE TRANSFORMATIONS (data shuffle required):\")\n",
    "wide_pipeline = parsed_transactions \\\n",
    "    .filter(lambda t: t['category'] == 'Electronics') \\\n",
    "    .map(lambda t: (t['customer_id'], t['amount'])) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .filter(lambda x: x[1] > 200) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Time wide operations\n",
    "start_time = time.time()\n",
    "wide_count = wide_pipeline.count()\n",
    "top_customer = wide_pipeline.first()\n",
    "wide_time = time.time() - start_time\n",
    "\n",
    "print(f\"   High-spending customers: {wide_count}\")\n",
    "print(f\"   Top customer: {top_customer[0]} with ${top_customer[1]:.2f}\")\n",
    "print(f\"   Execution time: {wide_time:.4f}s\")\n",
    "print(f\"   Partitions: {wide_pipeline.getNumPartitions()}\")\n",
    "\n",
    "# Analysis\n",
    "print(f\"\\nüìä PERFORMANCE ANALYSIS:\")\n",
    "print(f\"   Narrow transformations: {narrow_time:.4f}s\")\n",
    "print(f\"   Wide transformations: {wide_time:.4f}s\")\n",
    "print(f\"   Performance ratio: {wide_time/narrow_time:.1f}x slower\")\n",
    "\n",
    "# Show lineage complexity\n",
    "print(f\"\\nüîç LINEAGE COMPLEXITY:\")\n",
    "print(f\"   Narrow pipeline stages: {len(narrow_pipeline.toDebugString().decode('utf-8').split('|'))}\")\n",
    "print(f\"   Wide pipeline stages: {len(wide_pipeline.toDebugString().decode('utf-8').split('|'))}\")\n",
    "\n",
    "# Validation\n",
    "assert narrow_count > 0, \"Should have narrow transformation results\"\n",
    "assert wide_count > 0, \"Should have wide transformation results\"\n",
    "# Note: With small educational datasets, timing differences may be negligible or reversed due to overhead\n",
    "# In production with larger datasets, wide transformations typically take longer due to shuffle operations\n",
    "assert wide_time >= 0 and narrow_time >= 0, \"Both operations should complete successfully\"\n",
    "\n",
    "print(\"\\n‚úì Exercise 4.1 completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Common Patterns and Best Practices\n",
    "\n",
    "Learn to identify and optimize common transformation/action patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Avoiding Repeated Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad pattern: Multiple actions without caching\n",
    "print(\"‚ùå BAD PATTERN: Multiple actions without caching\")\n",
    "\n",
    "def bad_analysis_pattern():\n",
    "    # This RDD will be recomputed for each action!\n",
    "    analysis_rdd = parsed_transactions \\\n",
    "        .filter(lambda t: t['amount'] > 50) \\\n",
    "        .map(lambda t: (t['category'], t['amount']))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Each of these actions recomputes the entire chain\n",
    "    total_records = analysis_rdd.count()\n",
    "    category_totals = analysis_rdd.reduceByKey(lambda a, b: a + b).collect()\n",
    "    max_transaction = analysis_rdd.map(lambda x: x[1]).max()\n",
    "    sample_data = analysis_rdd.take(10)\n",
    "    \n",
    "    bad_time = time.time() - start_time\n",
    "    return bad_time, total_records, len(category_totals)\n",
    "\n",
    "bad_time, records, categories = bad_analysis_pattern()\n",
    "print(f\"   Time: {bad_time:.4f}s, Records: {records}, Categories: {categories}\")\n",
    "\n",
    "print(\"\\n‚úÖ GOOD PATTERN: Cache before multiple actions\")\n",
    "\n",
    "def good_analysis_pattern():\n",
    "    # Cache the RDD since we'll use it multiple times\n",
    "    analysis_rdd = parsed_transactions \\\n",
    "        .filter(lambda t: t['amount'] > 50) \\\n",
    "        .map(lambda t: (t['category'], t['amount'])) \\\n",
    "        .cache()  # Cache here!\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # First action computes and caches, others use cache\n",
    "    total_records = analysis_rdd.count()\n",
    "    category_totals = analysis_rdd.reduceByKey(lambda a, b: a + b).collect()\n",
    "    max_transaction = analysis_rdd.map(lambda x: x[1]).max()\n",
    "    sample_data = analysis_rdd.take(10)\n",
    "    \n",
    "    good_time = time.time() - start_time\n",
    "    \n",
    "    # Clean up\n",
    "    analysis_rdd.unpersist()\n",
    "    \n",
    "    return good_time, total_records, len(category_totals)\n",
    "\n",
    "good_time, records, categories = good_analysis_pattern()\n",
    "print(f\"   Time: {good_time:.4f}s, Records: {records}, Categories: {categories}\")\n",
    "\n",
    "improvement = ((bad_time - good_time) / bad_time) * 100\n",
    "print(f\"\\nüöÄ Caching improvement: {improvement:.1f}% faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Optimizing Transformation Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the importance of filter ordering\n",
    "print(\"üîß Optimizing transformation order\")\n",
    "\n",
    "# Inefficient: expensive operations before filtering\n",
    "print(\"\\n‚ùå INEFFICIENT: Complex operations before filtering\")\n",
    "start_time = time.time()\n",
    "inefficient = parsed_transactions \\\n",
    "    .map(lambda t: {\n",
    "        **t,\n",
    "        'tax': t['amount'] * 0.08,\n",
    "        'total_with_tax': t['amount'] * 1.08,\n",
    "        'category_upper': t['category'].upper()\n",
    "    }) \\\n",
    "    .filter(lambda t: t['amount'] > 200) \\\n",
    "    .filter(lambda t: t['category'] == 'Electronics')\n",
    "\n",
    "inefficient_result = inefficient.count()\n",
    "inefficient_time = time.time() - start_time\n",
    "print(f\"   Result: {inefficient_result} records in {inefficient_time:.4f}s\")\n",
    "\n",
    "# Efficient: filter first, then expensive operations\n",
    "print(\"\\n‚úÖ EFFICIENT: Filter first, then complex operations\")\n",
    "start_time = time.time()\n",
    "efficient = parsed_transactions \\\n",
    "    .filter(lambda t: t['category'] == 'Electronics') \\\n",
    "    .filter(lambda t: t['amount'] > 200) \\\n",
    "    .map(lambda t: {\n",
    "        **t,\n",
    "        'tax': t['amount'] * 0.08,\n",
    "        'total_with_tax': t['amount'] * 1.08,\n",
    "        'category_upper': t['category'].upper()\n",
    "    })\n",
    "\n",
    "efficient_result = efficient.count()\n",
    "efficient_time = time.time() - start_time\n",
    "print(f\"   Result: {efficient_result} records in {efficient_time:.4f}s\")\n",
    "\n",
    "optimization = ((inefficient_time - efficient_time) / inefficient_time) * 100\n",
    "print(f\"\\nüéØ Optimization gain: {optimization:.1f}% faster by filtering early\")\n",
    "\n",
    "assert inefficient_result == efficient_result, \"Both approaches should produce same results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.1**: Implement your own optimization comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Create two versions of the same analysis - unoptimized vs optimized\n",
    "\n",
    "print(\"üéØ Optimization Challenge: Customer Spending Analysis\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# UNOPTIMIZED VERSION\n",
    "print(\"\\n‚ùå UNOPTIMIZED VERSION:\")\n",
    "print(\"   - Complex transformations first\")\n",
    "print(\"   - No caching despite multiple actions\")\n",
    "print(\"   - Wide transformations before narrow filtering\")\n",
    "\n",
    "def unoptimized_analysis():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Bad: Complex operations first, then filtering\n",
    "    customer_analysis = parsed_transactions \\\n",
    "        .map(lambda t: (t['customer_id'], {\n",
    "            'amount': t['amount'],\n",
    "            'category': t['category'],\n",
    "            'tax_rate': 0.08 if t['category'] == 'Electronics' else 0.06,\n",
    "            'total_with_tax': t['amount'] * (1.08 if t['category'] == 'Electronics' else 1.06)\n",
    "        })) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda transactions: {\n",
    "            'total_spent': sum(t['total_with_tax'] for t in transactions),\n",
    "            'transaction_count': len(list(transactions))\n",
    "        }) \\\n",
    "        .filter(lambda x: x[1]['total_spent'] > 500) \\\n",
    "        .filter(lambda x: x[1]['transaction_count'] > 2)\n",
    "    \n",
    "    # Multiple actions without caching\n",
    "    high_spender_count = customer_analysis.count()\n",
    "    top_spender = customer_analysis.max(key=lambda x: x[1]['total_spent'])\n",
    "    avg_spending = customer_analysis.map(lambda x: x[1]['total_spent']).mean()\n",
    "    \n",
    "    unopt_time = time.time() - start_time\n",
    "    return unopt_time, high_spender_count, top_spender, avg_spending\n",
    "\n",
    "unopt_time, unopt_count, unopt_top, unopt_avg = unoptimized_analysis()\n",
    "print(f\"   Execution time: {unopt_time:.4f}s\")\n",
    "print(f\"   Results: {unopt_count} customers, Top: ${unopt_top[1]['total_spent']:.2f}, Avg: ${unopt_avg:.2f}\")\n",
    "\n",
    "# OPTIMIZED VERSION\n",
    "print(\"\\n‚úÖ OPTIMIZED VERSION:\")\n",
    "print(\"   - Same algorithm with caching for multiple actions\")\n",
    "print(\"   - Identical filtering and transformation logic\")\n",
    "\n",
    "def optimized_analysis():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Good: Same algorithm but with caching for multiple actions\n",
    "    customer_analysis = parsed_transactions \\\n",
    "        .map(lambda t: (t['customer_id'], {\n",
    "            'amount': t['amount'],\n",
    "            'category': t['category'],\n",
    "            'tax_rate': 0.08 if t['category'] == 'Electronics' else 0.06,\n",
    "            'total_with_tax': t['amount'] * (1.08 if t['category'] == 'Electronics' else 1.06)\n",
    "        })) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda transactions: {\n",
    "            'total_spent': sum(t['total_with_tax'] for t in transactions),\n",
    "            'transaction_count': len(list(transactions))\n",
    "        }) \\\n",
    "        .filter(lambda x: x[1]['total_spent'] > 500) \\\n",
    "        .filter(lambda x: x[1]['transaction_count'] > 2) \\\n",
    "        .cache()  # Cache here for multiple actions!\n",
    "    \n",
    "    # Multiple actions using cache\n",
    "    high_spender_count = customer_analysis.count()\n",
    "    top_spender = customer_analysis.max(key=lambda x: x[1]['total_spent'])\n",
    "    avg_spending = customer_analysis.map(lambda x: x[1]['total_spent']).mean()\n",
    "    \n",
    "    opt_time = time.time() - start_time\n",
    "    \n",
    "    # Clean up\n",
    "    customer_analysis.unpersist()\n",
    "    \n",
    "    return opt_time, high_spender_count, top_spender, avg_spending\n",
    "\n",
    "opt_time, opt_count, opt_top, opt_avg = optimized_analysis()\n",
    "print(f\"   Execution time: {opt_time:.4f}s\")\n",
    "print(f\"   Results: {opt_count} customers, Top: ${opt_top[1]['total_spent']:.2f}, Avg: ${opt_avg:.2f}\")\n",
    "\n",
    "# Performance comparison\n",
    "speedup = unopt_time / opt_time\n",
    "improvement = ((unopt_time - opt_time) / unopt_time) * 100\n",
    "\n",
    "print(f\"\\nüöÄ OPTIMIZATION RESULTS:\")\n",
    "print(f\"   Unoptimized time: {unopt_time:.4f}s\")\n",
    "print(f\"   Optimized time: {opt_time:.4f}s\")\n",
    "print(f\"   Speedup: {speedup:.1f}x faster\")\n",
    "print(f\"   Improvement: {improvement:.1f}%\")\n",
    "\n",
    "# Validation\n",
    "# Note: For small datasets, optimization overhead may sometimes exceed benefits\n",
    "# The key learning is understanding the optimization patterns\n",
    "if opt_time < unopt_time:\n",
    "    print(\"üöÄ Optimized version was faster as expected!\")\n",
    "else:\n",
    "    print(\"üìö Educational note: Small datasets may not show optimization benefits due to overhead\")\n",
    "    print(\"   In production with larger datasets, these optimizations provide significant improvements\")\n",
    "\n",
    "# Results should be exactly the same since we use identical algorithms, just with/without caching\n",
    "result_difference = abs(unopt_count - opt_count)\n",
    "assert result_difference == 0, f\"Results should be identical since same algorithm is used: difference of {result_difference}\"\n",
    "\n",
    "print(\"\\n‚úì Exercise 5.1 completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "Congratulations! You've mastered the distinction between transformations and actions in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Key Concepts Mastered:\n",
    "\n",
    "1. **Lazy Evaluation**: Transformations are lazy - they define computation graphs without executing\n",
    "2. **Eager Execution**: Actions trigger immediate execution of the entire transformation chain\n",
    "3. **Performance Impact**: Multiple actions without caching cause recomputation\n",
    "4. **Optimization Strategies**: \n",
    "   - Cache RDDs used by multiple actions\n",
    "   - Filter early to reduce data size\n",
    "   - Minimize shuffle operations\n",
    "   - Understand narrow vs wide transformations\n",
    "\n",
    "### üìä Transformations vs Actions Quick Reference:\n",
    "\n",
    "| **Transformations (Lazy)** | **Actions (Eager)** |\n",
    "|----------------------------|----------------------|\n",
    "| `map()`, `filter()`, `flatMap()` | `count()`, `collect()`, `first()` |\n",
    "| `reduceByKey()`, `groupByKey()` | `take()`, `reduce()`, `max()` |\n",
    "| `join()`, `union()`, `distinct()` | `saveAsTextFile()`, `foreach()` |\n",
    "| Return new RDD/DataFrame | Return values or write data |\n",
    "| Build execution graph | Trigger computation |\n",
    "\n",
    "### üöÄ Performance Best Practices:\n",
    "\n",
    "- **Cache strategically**: Use `.cache()` for RDDs accessed multiple times\n",
    "- **Filter early**: Reduce data size before expensive operations\n",
    "- **Avoid collect()**: Use `take()` or `sample()` for large datasets\n",
    "- **Monitor Spark UI**: Understand job execution and identify bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "spark.stop()\n",
    "print(\"\\nüéâ Lab 2 completed successfully!\")\n",
    "print(\"üìà You now understand the power of lazy evaluation in Spark!\")\n",
    "print(\"\\n‚û°Ô∏è  Next: Lab 3 - Lazy Evaluation Deep Dive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
