{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: DataFrame API Introduction - Solutions\n",
    "\n",
    "**Objective**: Master Spark's DataFrame API and understand its advantages over RDDs.\n",
    "\n",
    "**Learning Outcomes**:\n",
    "- Understand DataFrame structure and schema benefits\n",
    "- Convert between RDDs and DataFrames\n",
    "- Apply DataFrame transformations and actions\n",
    "- Leverage Catalyst optimizer advantages\n",
    "- Work with structured and semi-structured data\n",
    "\n",
    "**Estimated Time**: 45 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, avg, count, max as spark_max, when, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab4-DataFrame-API\") \\\n",
    "    .config(\"spark.sql.adaptive.logLevel\", \"ERROR\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1000\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")  # Suppress warnings for cleaner output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # Extra safety for log suppression\n",
    "\n",
    "print(f\"üöÄ Spark {spark.version} - DataFrame API Lab\")\n",
    "\n",
    "# Enhanced Spark UI URL display\n",
    "ui_url = spark.sparkContext.uiWebUrl\n",
    "print(f\"Spark UI: {ui_url}\")\n",
    "print(\"üí° In GitHub Codespaces: Check the 'PORTS' tab below for forwarded port 4040 to access Spark UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: DataFrame Fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data as DataFrames\n",
    "customers_df = spark.read.csv(\"../Datasets/customers.csv\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(\"../Datasets/customer_transactions.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"üìä DataFrames loaded with inferred schemas:\")\n",
    "customers_df.printSchema()\n",
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1**: Create DataFrames with explicit schemas and compare with RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Create explicit schema for transactions\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True),\n",
    "    StructField(\"is_weekend\", StringType(), True),\n",
    "    StructField(\"discount_applied\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Solution: Load with explicit schema\n",
    "transactions_explicit = spark.read.csv(\n",
    "    \"../Datasets/customer_transactions.csv\", \n",
    "    header=True, \n",
    "    schema=transaction_schema\n",
    ")\n",
    "\n",
    "# Solution: Compare DataFrame vs RDD operations\n",
    "# DataFrame approach\n",
    "df_high_value = transactions_explicit.filter(col(\"amount\") > 100).select(\"customer_id\", \"amount\")\n",
    "\n",
    "# RDD approach  \n",
    "def parse_transaction(line):\n",
    "    fields = line.split(',')\n",
    "    return (fields[1], float(fields[2]))  # customer_id, amount\n",
    "\n",
    "rdd_high_value = spark.sparkContext.textFile(\"../Datasets/customer_transactions.csv\") \\\n",
    "    .filter(lambda line: not line.startswith('transaction_id')) \\\n",
    "    .map(parse_transaction) \\\n",
    "    .filter(lambda x: x[1] > 100)\n",
    "\n",
    "# Solution: Time both approaches\n",
    "print(\"‚è±Ô∏è  Performance comparison:\")\n",
    "\n",
    "start_time = time.time()\n",
    "df_count = df_high_value.count()\n",
    "df_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "rdd_count = rdd_high_value.count()\n",
    "rdd_time = time.time() - start_time\n",
    "\n",
    "print(f\"DataFrame approach: {df_count} records in {df_time:.4f}s\")\n",
    "print(f\"RDD approach: {rdd_count} records in {rdd_time:.4f}s\")\n",
    "print(f\"Performance ratio: {rdd_time/df_time:.1f}x (DataFrame advantage)\")\n",
    "\n",
    "assert df_high_value.count() == rdd_high_value.count(), \"Both should return same count\"\n",
    "print(\"‚úì Exercise 1.1 completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: DataFrame Operations and SQL Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrames as temp views for SQL\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "# DataFrame API vs SQL comparison\n",
    "print(\"üìä DataFrame API vs SQL Analysis\")\n",
    "\n",
    "# DataFrame API approach\n",
    "df_analysis = transactions_df.groupBy(\"category\") \\\n",
    "    .agg(spark_sum(\"amount\").alias(\"total_amount\"), \n",
    "         avg(\"amount\").alias(\"avg_amount\"), \n",
    "         count(\"*\").alias(\"transaction_count\")) \\\n",
    "    .orderBy(col(\"total_amount\").desc())\n",
    "\n",
    "# SQL approach\n",
    "sql_analysis = spark.sql(\"\"\"\n",
    "    SELECT category,\n",
    "           SUM(amount) as total_amount,\n",
    "           AVG(amount) as avg_amount,\n",
    "           COUNT(*) as transaction_count\n",
    "    FROM transactions\n",
    "    GROUP BY category\n",
    "    ORDER BY total_amount DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"DataFrame API results:\")\n",
    "df_analysis.show()\n",
    "print(\"SQL results:\")\n",
    "sql_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.1**: Build complex queries using both DataFrame API and SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Customer analysis - high-value customers by state\n",
    "# Requirements: Join customers and transactions, calculate total spending per customer,\n",
    "# classify as high-value (>$1000), group by state, show counts and averages\n",
    "\n",
    "print(\"üéØ Complex Analysis Challenge\")\n",
    "\n",
    "# DataFrame API approach\n",
    "df_customer_analysis = transactions_df.join(customers_df, \"customer_id\") \\\n",
    "    .groupBy(\"customer_id\", \"state\") \\\n",
    "    .agg(spark_sum(\"amount\").alias(\"total_spent\")) \\\n",
    "    .withColumn(\"is_high_value\", when(col(\"total_spent\") > 1000, 1).otherwise(0)) \\\n",
    "    .groupBy(\"state\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_customers\"),\n",
    "        spark_sum(\"is_high_value\").alias(\"high_value_customers\"),\n",
    "        avg(\"total_spent\").alias(\"avg_spending_per_customer\")\n",
    "    ) \\\n",
    "    .withColumn(\"high_value_percentage\", \n",
    "               (col(\"high_value_customers\") / col(\"total_customers\") * 100)) \\\n",
    "    .orderBy(col(\"high_value_percentage\").desc())\n",
    "\n",
    "# SQL approach\n",
    "sql_customer_analysis = spark.sql(\"\"\"\n",
    "    WITH customer_spending AS (\n",
    "        SELECT \n",
    "            c.customer_id,\n",
    "            c.state,\n",
    "            SUM(t.amount) as total_spent\n",
    "        FROM customers c\n",
    "        JOIN transactions t ON c.customer_id = t.customer_id\n",
    "        GROUP BY c.customer_id, c.state\n",
    "    ),\n",
    "    state_summary AS (\n",
    "        SELECT \n",
    "            state,\n",
    "            COUNT(*) as total_customers,\n",
    "            SUM(CASE WHEN total_spent > 1000 THEN 1 ELSE 0 END) as high_value_customers,\n",
    "            AVG(total_spent) as avg_spending_per_customer\n",
    "        FROM customer_spending\n",
    "        GROUP BY state\n",
    "    )\n",
    "    SELECT \n",
    "        *,\n",
    "        (high_value_customers * 100.0 / total_customers) as high_value_percentage\n",
    "    FROM state_summary\n",
    "    ORDER BY high_value_percentage DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"DataFrame API approach:\")\n",
    "df_customer_analysis.show()\n",
    "\n",
    "print(\"SQL approach:\")\n",
    "sql_customer_analysis.show()\n",
    "\n",
    "# Solution: Verify both produce same results\n",
    "df_count = df_customer_analysis.count()\n",
    "sql_count = sql_customer_analysis.count()\n",
    "assert df_count == sql_count, \"Both approaches should return same number of states\"\n",
    "\n",
    "print(\"‚úì Exercise 2.1 completed!\")\n",
    "print(f\"Both approaches analyzed {df_count} states with identical results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Advanced DataFrame Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window functions and advanced operations\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank\n",
    "\n",
    "# Customer ranking within each state\n",
    "customer_totals = transactions_df.join(customers_df, \"customer_id\") \\\n",
    "    .groupBy(\"customer_id\", \"name\", \"state\") \\\n",
    "    .agg(spark_sum(\"amount\").alias(\"total_spent\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"state\").orderBy(col(\"total_spent\").desc())\n",
    "\n",
    "ranked_customers = customer_totals.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"rank\") <= 3)\n",
    "\n",
    "print(\"üèÜ Top 3 customers per state:\")\n",
    "ranked_customers.orderBy(\"state\", \"rank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1**: Implement advanced DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Advanced analytics with window functions\n",
    "# 1. Calculate running totals by customer\n",
    "# 2. Find month-over-month growth rates\n",
    "# 3. Identify anomaly transactions (significantly above customer's average)\n",
    "\n",
    "from pyspark.sql.functions import month, year, lag, when, sum as spark_sum, avg as spark_avg\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Solution: Create date-based analysis\n",
    "transactions_with_date = transactions_df.withColumn(\"year\", year(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"transaction_date\")))\n",
    "\n",
    "# Solution: Monthly customer spending\n",
    "monthly_spending = transactions_with_date.groupBy(\"customer_id\", \"year\", \"month\") \\\n",
    "    .agg(spark_sum(\"amount\").alias(\"monthly_total\"))\n",
    "\n",
    "# Solution: Add running total and growth rate using window functions\n",
    "customer_window = Window.partitionBy(\"customer_id\").orderBy(\"year\", \"month\")\n",
    "\n",
    "customer_trends = monthly_spending \\\n",
    "    .withColumn(\"running_total\", \n",
    "               spark_sum(\"monthly_total\").over(customer_window.rowsBetween(Window.unboundedPreceding, Window.currentRow))) \\\n",
    "    .withColumn(\"prev_month_total\", \n",
    "               lag(\"monthly_total\").over(customer_window)) \\\n",
    "    .withColumn(\"month_over_month_growth\",\n",
    "               when(col(\"prev_month_total\").isNull(), 0.0)\n",
    "               .otherwise((col(\"monthly_total\") - col(\"prev_month_total\")) / col(\"prev_month_total\") * 100))\n",
    "\n",
    "# Solution: Show results\n",
    "print(\"üìà Customer spending trends:\")\n",
    "customer_trends.filter(col(\"customer_id\") == \"CUST_000001\") \\\n",
    "    .orderBy(\"year\", \"month\") \\\n",
    "    .show()\n",
    "\n",
    "# Additional analysis: Identify high-growth customers\n",
    "high_growth_customers = customer_trends \\\n",
    "    .filter(col(\"month_over_month_growth\") > 50) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"high_growth_months\"),\n",
    "        avg(\"month_over_month_growth\").alias(\"avg_growth_rate\")\n",
    "    ) \\\n",
    "    .filter(col(\"high_growth_months\") >= 2) \\\n",
    "    .orderBy(col(\"avg_growth_rate\").desc())\n",
    "\n",
    "print(\"\\nüöÄ High-growth customers (>50% growth for 2+ months):\")\n",
    "high_growth_customers.show(10)\n",
    "\n",
    "# Validation\n",
    "trends_count = customer_trends.count()\n",
    "growth_count = high_growth_customers.count()\n",
    "assert trends_count > 0, \"Should have customer trends\"\n",
    "assert growth_count >= 0, \"Should have high-growth analysis (may be 0)\"\n",
    "\n",
    "print(\"‚úì Exercise 3.1 completed!\")\n",
    "print(f\"Analyzed {trends_count} customer-month combinations\")\n",
    "print(f\"Identified {growth_count} high-growth customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: DataFrame API Benefits\n",
    "\n",
    "### Key Advantages:\n",
    "1. **Schema Awareness**: Type safety and optimization\n",
    "2. **Catalyst Optimizer**: Automatic query optimization  \n",
    "3. **SQL Integration**: Familiar syntax for analysts\n",
    "4. **Performance**: Better than RDDs for structured data\n",
    "5. **Expressiveness**: Rich API for complex operations\n",
    "\n",
    "### When to Use DataFrames vs RDDs:\n",
    "- **DataFrames**: Structured/semi-structured data, SQL-like operations, performance critical\n",
    "- **RDDs**: Unstructured data, complex transformations, functional programming style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"üéâ Lab 4 completed! DataFrame API mastered.\")\n",
    "print(\"‚û°Ô∏è  Next: Lab 5 - Spark SQL Basics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
